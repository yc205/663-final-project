{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'%.4f'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as ss\n",
    "from functools import partial\n",
    "from pandas import Series, DataFrame, Panel\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from numba import jit, int32, int64, float32, float64 \n",
    "\n",
    "import multiprocessing\n",
    "%matplotlib inline\n",
    "%precision 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython\n",
    "from rpy2.robjects.packages import importr\n",
    "p1=importr('leaps')\n",
    "p2=importr('stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cythonmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext cythonmagic\n"
     ]
    }
   ],
   "source": [
    "%load_ext cythonmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I selected the Fast FSR Variable Selection research paper by Yujun Wu, Leonard A. Stefanski and Dennis D. Boos in 2009. Many variable selection procedures have been developed in the literature for linear regression models. A new and general approach, the False Selection Rate (FSR) method, to control variable selection is applicable to a broader class of regression models. The algorithm Fast FSR is a type of forward selection method and sequentially selects variables with fixed False Selection Rate (Usually target rate 0.05). \n",
    "   \n",
    "   The earlier verison of FSR variable selection method by Wu, Boos, and Stefanski (2007) requires\n",
    "the generation of the phony explanatory variables and the rate at which they enter a variable selection procedure is monitored as a function of a tuning parameter like α-to-enter of forward selection. This rate function is then used to estimate the appropriate tuning parameter so that the average rate that uninformative variables enter selected models is controlled to be γ0, usually 0.05. However, the Fast FSR developed in this paper requires no phony variable generation, but achieves the same result. Bascially, it depends on this mathematical formula to select variables:\n",
    "     $$ K(\\gamma_0) = max \\{i :\\tilde{p_i} <= \\frac{(1 + S)* \\gamma_0}{k_{T} - S}, and, \\tilde{p_i} <= \\alpha_{max}\\}$$\n",
    "   \n",
    "   Fast FSR has competitive advantages among model selection method. It can give the parsimony model when the number of variables are bigger than the number of observations. Although lasso regression performs well in high dimension model selection, it is not competitive based on some criteria, where Fast FSR can compensate these disadvantages. Fast FSR has lower False Selection Rate than Lasso regression and have similar Model Error as lasso, which will be shown in the simulation study part. In addition, I implemented the optimization of the Fast FSR, Fast FSR bagging which is useful when a large of high correlated predictors are suspected in real case. Normal forward selection’s and lasso prediction performance can degrade with higly correlated predictors. \n",
    "   \n",
    " - Flow of this project\n",
    "     - Implement the Fast FSR and bagging Fast FSR\n",
    "     - Two unit tests on the forward selection and get_fsr functions\n",
    "     - Profile the performance of the algorithm: Pure python and vectorized python of function are written and the vectorized version improved the speed twice\n",
    "     - High performance programming : Parellel programming by using multiple core. The result improves the speed twice compared the vectorized version.\n",
    "     - Application and comparison: Compare the lasso and Fast FSR on the four simulated data set models. Based on the two criteria, False selection rate and Model error. Lasso and Fast FSR has similar Model error, however, the False Selection rate is lower in Fast FSR\n",
    "     - Method optimization: Bagging Fast FSR: more applicable for the general data set: highly correlated predictors are suspected.\n",
    "     - Reproducible analysis: I applied the Fast FSR to the NCAA data which is provided by the \"Boos-Stefanski Variable Selection Home\"\n",
    "     http://www4.stat.ncsu.edu/~boos/var.select/ncaa.data.orig.txt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Implement main functions: Fast FSR\n",
    "  - Description of fsr_fast_vectorized:\n",
    "     - Input observation array and reponse y\n",
    "     - Based on the forward selection function and selection rule as follows, variables are selected\n",
    "     $$ K(\\gamma_0) = max \\{i :\\tilde{p_i} <= \\frac{(1 + S)* \\gamma_0}{k_{T} - S}, and, \\tilde{p_i} <= \\alpha_{max}\\}$$\n",
    "     - Returned linear regression model on the selected variables, model size, name of the selected variables and false selection rate.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fsr_fast_vectorized(x,y):\n",
    "    gam0=0.05\n",
    "    digits = 4\n",
    "    m = x.shape[1]\n",
    "    n = x.shape[0]\n",
    "    if(m >= n): \n",
    "        m1=n-5  \n",
    "    else: \n",
    "        m1=m \n",
    "    vm = range(m1)\n",
    "    pvm = np.zeros(m1)  # to create pvm below\n",
    "    out_x = p1.regsubsets(x,y,method=\"forward\")  # foward selection by r function\n",
    "    rss = out_x[9]\n",
    "    nn = out_x[26][0]\n",
    "    n_rss = np.array(range(len(rss)-1))\n",
    "    q = [(rss[i]-rss[i+1])*(nn-i-2)/rss[i+1] for i in n_rss]\n",
    "    rvf = ss.f(1,nn-n_rss-2)\n",
    "    orig = np.array(1-rvf.cdf(q))    \n",
    "    for i in range(m1):  # sequentially get max of pvalues\n",
    "        pvm[i] = np.max(orig[0:i+1])\n",
    "    alpha = [0]+pvm\n",
    "    S = np.zeros(len(alpha)) # Include number of true entering in orig.\n",
    "    for ia in range(1,len(alpha)):   #loop through alpha values for S=size and size of models at alpha[ia], S[1]=0                 \n",
    "        S[ia] = sum([pvm[i] <= alpha[ia] for i in range(len(pvm))])        \n",
    "    ghat = (m-S)*alpha/(1+S)    \n",
    "    alphamax = alpha[np.argmax(ghat)] # Got index of largest ghat\n",
    "    ind = np.zeros(len(ghat))\n",
    "    ind = np.where((ghat<gam0)&(alpha <=alphamax),1,0)\n",
    "    Sind = S[np.max(np.where(np.array(ind)>0))] # model size with ghat just below gam0\n",
    "    alphahat_fast = (1+Sind)*gam0/(m-Sind) \n",
    "    size1=np.sum(np.array(pvm)<=alphahat_fast)+1 # size of model including intercept\n",
    "    x=x[list(x.columns.values[list((np.array(out_x[7])-2)[1:size1])])]\n",
    "    x=sm.add_constant(x) # linear regression on the selected variables\n",
    "    if(size1>1): \n",
    "        x_ind=(np.array(out_x[7])-1)[1:size1]\n",
    "    else:\n",
    "        x_ind=0\n",
    "    if (size1==1):\n",
    "        mod = np.mean(y)\n",
    "    else:\n",
    "        mod = sm.OLS(y, x).fit()\n",
    " \n",
    "    return mod,size1-1,x_ind,alphahat_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FSRR_vectorized function description:\n",
    "   - This function returns the false selection rates for n iterations. \n",
    "   - Input: - target: true predictors under the simulated model. method: lasso variable    selection or Fast FSR model selection. model : four true models can be selected. n : the number of iterations. \n",
    "   - By calling model number,the corresponding true model is generated with observation matrix and reponse variables. Then, it will generated all quadritic term for all the variables. By applying the lasso or Fast FSR, it will select some important variables. Then,get_fsr function can calculated the false selection rate by comparing the true variables and the selected variables. \n",
    "   - Saved each iteration false selection rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FSRR_vectorized (target,method,model,n):\n",
    "    l =[]\n",
    "    for i in range(n):\n",
    "        x = np.array(np.random.normal(1, 1, 21*1500).reshape(1500,21))\n",
    "        if (model==1):\n",
    "            y = x[:,0]\n",
    "        if (model==2):\n",
    "            b = np.array([9,4,1,9,4,1])\n",
    "            y = np.dot(x[:,0:6],b)\n",
    "        if (model==3):\n",
    "            b = np.array([25,16,9,4,1,25,16,9,4,1])\n",
    "            y = np.dot(x[:,0:10],b)\n",
    "        if (model==4):\n",
    "            b = np.array([45,36,25,16,9,4,1,45,36,25,16,9,4,1])\n",
    "            y = np.dot(x[:,0:14],b)\n",
    "        if (model==5):\n",
    "            x[:,2] = x[:,3]\n",
    "            x[:,4] = x[:,5]\n",
    "            b = np.array([9,4,1,9,4,1])\n",
    "            y = np.dot(x[:,0:6],b)\n",
    "        quad = (x[:,0:20])**2\n",
    "        x = np.concatenate((x,quad),axis=1)\n",
    "        x = pd.DataFrame(x)\n",
    "        m = method(x,y)\n",
    "        L = get_fsr(target,method,x,y)\n",
    "        l.append(L)\n",
    "    return l\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Unit Tests on functions forward_selection and get_fsr\n",
    " \n",
    "   - Unit test of forward_selection: The true linear relationship should be y = b + 10*x1+ 200*x2+ 0.5*x3 + 0.01*x4 + 0.001*x5. This forward selection function should select variables from the most related to less related. Thus, the returned result should be 1,3,2,4,5,6 where 1 denotes the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1 3 2 4 5 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def foward_selection(x,y):\n",
    "    out_x = p1.regsubsets(x,y,method=\"forward\") \n",
    "    rss = out_x[9]\n",
    "    nn = out_x[26][0]\n",
    "    r_7 = out_x[7]\n",
    "    q = [(rss[i]-rss[i+1])*(nn-i-2)/rss[i+1] for i in range(len(rss)-1)]\n",
    "    rvf = [ ss.f(1,nn-i-2)  for i in range(len(rss)-1)]\n",
    "    orig =  [1-rvf[i].cdf(q[i]) for i in range(len(rss)-1)]\n",
    "    return orig,r_7\n",
    "x = pd.DataFrame(np.random.normal(1, 1,5*1000).reshape(1000,5))\n",
    "b = np.array([10,200,0.5,0.01,0.001])\n",
    "y = np.dot(x,b)\n",
    "print foward_selection(x,y)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fsr (target,method,x,y):  \n",
    "        m = method(x,y)\n",
    "        if (len(m) == 4 and m[1]==0):\n",
    "             m = []\n",
    "        if (len(m) == 4 and m[1]!=0):\n",
    "             m = m[2]\n",
    "        I = set(target)&set(m)\n",
    "        L = (len(m)-len(I))/(1.0+len(m))\n",
    "        return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling: Naive Version and Vectoried Verison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - First two naive version functions are written. Then I profile the naive functions and find that it is unnecessary to do the Cythoned verion. So, I did the vectorized version of the functions. The speed of the vectorized verision improves twice as much as the pure python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install --pre line-profiler &> /dev/null\n",
    "! pip install psutil &> /dev/null\n",
    "! pip install memory_profiler &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Naive_Fast_FSR(x,y):\n",
    "    gam0=0.05\n",
    "    digits = 4\n",
    "    pl = 1\n",
    "    m = x.shape[1]\n",
    "    n = x.shape[0]\n",
    "    if(m >= n): \n",
    "        m1=n-5  \n",
    "    else: \n",
    "        m1=m \n",
    "    vm = range(m1)\n",
    "  # if only partially named columns corrects for no colnames\n",
    "    pvm = [0] * m1 \n",
    "    out_x = p1.regsubsets(x,y,method=\"forward\")  \n",
    "    rss = out_x[9]\n",
    "    nn = out_x[26][0]\n",
    "    q = [(rss[i]-rss[i+1])*(nn-i-2)/rss[i+1] for i in range(len(rss)-1)]\n",
    "    rvf = [ ss.f(1,nn-i-2)  for i in range(len(rss)-1)]\n",
    "    orig =  [1-rvf[i].cdf(q[i]) for i in range(len(rss)-1)]\n",
    "# sequential max of pvalues\n",
    "    for i in range(m1):\n",
    "        pvm[i] = max(orig[0:i+1])  \n",
    "    alpha = [0]+pvm\n",
    "    ng = len(alpha)\n",
    " # will contain num. of true entering in orig\n",
    "    S = [0] * ng\n",
    " # loop through alpha values for S=size                        \n",
    "    for ia in range(1,ng):                   \n",
    "        S[ia] = sum([pvm[i] <= alpha[ia] for i in range(len(pvm))])        # size of models at alpha[ia], S[1]=0\n",
    "    ghat = [(m-S[i])*alpha[i]/(1+S[i]) for i in range(len(alpha))]              # gammahat_ER \n",
    "    alphamax = alpha[np.argmax(ghat)]\n",
    "    ind = [0]*len(ghat)\n",
    "    ind = [ 1 if ghat[i]<gam0 and alpha[i]<=alphamax else 0 for i in range(len(ghat))]\n",
    "    Sind = S[np.max(np.where(np.array(ind)>0))]\n",
    "    alphahat_fast = (1+Sind)*gam0/(m-Sind)\n",
    "    size1=np.sum(np.array(pvm)<=alphahat_fast)+1\n",
    "    x=x[list(x.columns.values[list((np.array(out_x[7])-2)[1:size1])])]\n",
    "    x=sm.add_constant(x)\n",
    "    if(size1>1): \n",
    "        x_ind=(np.array(out_x[7])-1)[1:size1]\n",
    "    else:\n",
    "        x_ind=0\n",
    "    if (size1==1):\n",
    "        mod = np.mean(y)\n",
    "    else:\n",
    "        mod = sm.OLS(y, x).fit()\n",
    "    return mod,size1-1,x_ind,alphahat_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Naive_FSRR (target,method,model,n):\n",
    "    l =[]\n",
    "    for i in range(n):\n",
    "        x = pd.DataFrame(np.random.normal(1, 1, 21*1500).reshape(1500,21))\n",
    "        if (model==1):\n",
    "            y = x.ix[:,1]\n",
    "        if (model==2):\n",
    "            y = 9*x.ix[:,0]+4*x.ix[:,1]+x.ix[:,2]+9*x.ix[:,3]+4*x.ix[:,4]+x.ix[:,5]\n",
    "        if (model==3):\n",
    "            y = 25*x.ix[:,0]+16*x.ix[:,1]+9*x.ix[:,2]+4*x.ix[:,3]+1*x.ix[:,4]+25*x.ix[:,5]+16*x.ix[:,6]+9*x.ix[:,7]+4*x.ix[:,8]+1*x.ix[:,9]\n",
    "        if (model==4):\n",
    "            y = 45*x.ix[:,0]+36*x.ix[:,1]+25*x.ix[:,2]+16*x.ix[:,3]+9*x.ix[:,4]+4*x.ix[:,5]+x.ix[:,6]+45*x.ix[:,7]+36*x.ix[:,8]+25*x.ix[:,9]+16*x.ix[:,10]+9*x.ix[:,11]+4*x.ix[:,12]+x.ix[:,13]\n",
    "        if (model==5):\n",
    "            x.ix[:,2] = 2*x.ix[:,3]\n",
    "            x.ix[:,4] = 3*x.ix[:,5]\n",
    "            y = 9*x.ix[:,5]+4*x.ix[:,6]+x.ix[:,7]+9*x.ix[:,12]+4*x.ix[:,13]+x.ix[:,14]\n",
    "        quad = (x.ix[:,0:20])**2\n",
    "        x = np.concatenate((x,quad),axis=1)\n",
    "        x = pd.DataFrame(x)\n",
    "        m = method(x,y)\n",
    "        L = get_fsr(target,method,x,y)\n",
    "        l.append(L)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = pd.DataFrame(np.random.normal(1, 1, 21*150000).reshape(150000,21))\n",
    "y = 45*x.ix[:,0]+36*x.ix[:,1]+25*x.ix[:,2]+16*x.ix[:,3]+9*x.ix[:,4]+4*x.ix[:,5]+x.ix[:,6]+45*x.ix[:,7]+36*x.ix[:,8]+25*x.ix[:,9]+16*x.ix[:,10]+9*x.ix[:,11]+4*x.ix[:,12]+x.ix[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%lprun -f fsr_fast_vectorized fsr_fast_vectorized(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -f Naive_Fast_FSR Naive_Fast_FSR(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%lprun -f FSRR_vectorized FSRR_vectorized([0,1,2,3,4,5,6,7,8,9,10,11,12,13],fsr_fast_vectorized,4,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -f Naive_FSRR Naive_FSRR([0,1,2,3,4,5,6,7,8,9,10,11,12,13],Naive_Fast_FSR,4,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Time comparsion for pure python and vectorized python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 17.7 s per loop\n"
     ]
    }
   ],
   "source": [
    "n =100\n",
    "%timeit Naive_FSRR([0,1,2,3,4,5,6,7,8,9,10,11,12,13],Naive_Fast_FSR,4,n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 9.15 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit FSRR_vectorized([0,1,2,3,4,5,6,7,8,9,10,11,12,13],fsr_fast_vectorized,4,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Performance : Parallel Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this part, I use the parallel programming and it improves the speed twice as much as vectorized python and fourth times as the pure python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pi_multiprocessing1(target,method,model,n):\n",
    "    \"\"\"Split a job of length n into num_procs pieces.\"\"\"\n",
    "    import multiprocessing\n",
    "    m = multiprocessing.cpu_count()\n",
    "    pool = multiprocessing.Pool(m)\n",
    "    mapfunc = partial(FSRR_vectorized,target,method,model)\n",
    "    results = pool.map(mapfunc,[n/m]*m)\n",
    "    pool.close()\n",
    "    return np.mean(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The multiple core programming based on the vectorized python improves the speed twice compared to the vectorized python and four times compared to he pure python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 4.62 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit pi_multiprocessing1([0,1,2,3,4,5,6,7,8,9,10,11,12,13],fsr_fast_vectorized,4,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparsion among lasso, Fast_FSR based on two criteria: Model Error Ratio and False Selection Rate by the simulated data \n",
    "- In this simulation study, I simulated 100 data points with 42 variables. Four models are simulated: H1: First variable is non-zero. H2: 6 variables are non-zeros at variables 1-6 with values (9,4,1,9,4,1). H3: 10 variables are non-zeros at variables 1-9 with values (25,16,9,4,1,25,16,9,4,1). H4: 14 variables are non-zeros at variables 1-14 with value (49, 36, 25, 16, 9, 4, 1,49, 36, 25, 16, 9, 4, 1)\n",
    "- Result: Under each model, False Selection rate of Fast_FSR is higher than that of Lasso and the Model Error for lasso and Fast_FSR are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lasso_fit (x,y):\n",
    "    alpha =0.5\n",
    "    lasso = Lasso(alpha=alpha, tol=0.001)\n",
    "    y_coef_lasso = lasso.fit(x, y).coef_\n",
    "    lasso_index = np.where(y_coef_lasso != 0)[0]+1\n",
    "    return lasso_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO FSR is 0.443333333333\n",
      "FAST FSR is 0.0833333333333\n"
     ]
    }
   ],
   "source": [
    "target =[1]\n",
    "n = int(100)\n",
    "\n",
    "print \"LASSO FSR is\",pi_multiprocessing1(target,lasso_fit,1,n)\n",
    "print \"FAST FSR is\",pi_multiprocessing1(target,fsr_fast_vectorized,1,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO FSR is 0.540920745921\n",
      "FAST FSR is 0.189285714286\n"
     ]
    }
   ],
   "source": [
    "n = int(100)\n",
    "target = [0,1,2,3,4,5]\n",
    "\n",
    "print \"LASSO FSR is\",pi_multiprocessing1(target,lasso_fit,2,n)\n",
    "print \"FAST FSR is\",pi_multiprocessing1(target,fsr_fast_vectorized,2,n)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO FSR is 0.526709273183\n",
      "FAST FSR is 0.138478188478\n"
     ]
    }
   ],
   "source": [
    "n = int(100)\n",
    "target = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "print \"LASSO FSR is\",pi_multiprocessing1(target,lasso_fit,3,n)\n",
    "print \"FAST FSR is\",pi_multiprocessing1(target,fsr_fast_vectorized,3,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO FSR is 0.517102718482\n",
      "FAST FSR is 0.112006535948\n"
     ]
    }
   ],
   "source": [
    "n = int(100)\n",
    "target = [0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "print \"LASSO FSR is\",pi_multiprocessing1(target,lasso_fit,4,n)\n",
    "print \"FAST FSR is\",pi_multiprocessing1(target,fsr_fast_vectorized,4,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extension for Fast_FSR: Bagging Fast FSR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging FSR: As you can see, if you use the normal Fast FSR, the rate will be very high. However, by implementing the Bagging Fast FSR,it will improve the False Selection rate to the target level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_fsr(x,y,B,gam0,method,digits):\n",
    "    m = x.shape[1]\n",
    "    n = x.shape[0]\n",
    "    hold = np.zeros((B,m+1))      # holds coefficients\n",
    "    hold = pd.DataFrame(hold)\n",
    "    alphahat = [0] * B                    # holds alphahats\n",
    "    size = [0] * B\n",
    "    for i in range(B):\n",
    "        index = np.random.choice(n, n)\n",
    "        out = method(x.ix[index,:],y.ix[index])\n",
    "        if out[1]>0:\n",
    "            hold.iloc[i,out[2]] = np.array(out[0].params)[1:(len(out[2])+1)]\n",
    "        hold.iloc[i,0] = out[0].params[0]\n",
    "        alphahat[i] = out[3]\n",
    "        size[i] = out[1]\n",
    "    hold[np.isnan(hold)]=0\n",
    "    para_av = np.mean(hold,0)\n",
    "    para_sd = [0]*(m+1)\n",
    "    para_sd = np.var(hold,0)**0.5\n",
    "    amean = np.mean(alphahat)\n",
    "    sizem = np.mean(size)\n",
    "    pred = np.matrix(x)*np.transpose(np.matrix(para_av[1:]))+para_av[0]\n",
    "    return para_av,para_sd,pred,amean,sizem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reordering variables and trying again:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7500"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.DataFrame(np.random.normal(1, 1, 21*1500).reshape(1500,21))\n",
    "y = x.ix[:,0]+2*x.ix[:,1]\n",
    "x.ix[:,3]= x.ix[:,0]\n",
    "get_fsr ([0,1],fsr_fast_vectorized,x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n",
      "Reordering variables and trying again:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0     0.746201\n",
       " 1     0.535000\n",
       " 2     2.003511\n",
       " 3    -0.000070\n",
       " 4     0.245000\n",
       " 5     0.000919\n",
       " 6    -0.003615\n",
       " 7    -0.003380\n",
       " 8     0.002279\n",
       " 9    -0.001958\n",
       " 10    0.001221\n",
       " 11   -0.004208\n",
       " 12   -0.001020\n",
       " 13    0.000113\n",
       " 14   -0.000528\n",
       " 15    0.001096\n",
       " 16    0.001677\n",
       " 17    0.001543\n",
       " 18    0.002455\n",
       " 19   -0.000005\n",
       " 20    0.003468\n",
       " 21    0.002427\n",
       " dtype: float64, 0     0.346211\n",
       " 1     0.431596\n",
       " 2     0.012387\n",
       " 3     0.005481\n",
       " 4     0.349964\n",
       " 5     0.006129\n",
       " 6     0.013239\n",
       " 7     0.011831\n",
       " 8     0.008719\n",
       " 9     0.010197\n",
       " 10    0.010997\n",
       " 11    0.015030\n",
       " 12    0.006239\n",
       " 13    0.007332\n",
       " 14    0.007100\n",
       " 15    0.009812\n",
       " 16    0.008185\n",
       " 17    0.008579\n",
       " 18    0.010391\n",
       " 19    0.006459\n",
       " 20    0.012542\n",
       " 21    0.015034\n",
       " dtype: float64, matrix([[ 7.7414],\n",
       "         [ 2.0401],\n",
       "         [ 7.529 ],\n",
       "         ..., \n",
       "         [ 6.5569],\n",
       "         [ 3.021 ],\n",
       "         [ 5.1156]]), 0.0515, 9.2900)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = 100\n",
    "bag_fsr(x,y,B,0.05,fsr_fast_vectorized,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducible analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NCCA data: Result here is same as the result on the original website which can be refered to \n",
    "http://www4.stat.ncsu.edu/~boos/var.select/fsr.fast.ncaa.ex.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.811</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.800</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   75.50</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 30 Apr 2015</td> <th>  Prob (F-statistic):</th> <td>2.49e-30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:09:42</td>     <th>  Log-Likelihood:    </th> <td> -315.88</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    94</td>      <th>  AIC:               </th> <td>   643.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    88</td>      <th>  BIC:               </th> <td>   659.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>  -42.1069</td> <td>    8.990</td> <td>   -4.684</td> <td> 0.000</td> <td>  -59.972   -24.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    3.4714</td> <td>    0.467</td> <td>    7.428</td> <td> 0.000</td> <td>    2.543     4.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.2391</td> <td>    0.076</td> <td>    3.163</td> <td> 0.002</td> <td>    0.089     0.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.2787</td> <td>    0.078</td> <td>    3.582</td> <td> 0.001</td> <td>    0.124     0.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.6770</td> <td>    0.195</td> <td>    3.475</td> <td> 0.001</td> <td>    0.290     1.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   -2.5913</td> <td>    0.832</td> <td>   -3.115</td> <td> 0.002</td> <td>   -4.245    -0.938</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 5.624</td> <th>  Durbin-Watson:     </th> <td>   1.718</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.060</td> <th>  Jarque-Bera (JB):  </th> <td>   3.905</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.351</td> <th>  Prob(JB):          </th> <td>   0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.290</td> <th>  Cond. No.          </th> <td>    620.</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.811\n",
       "Model:                            OLS   Adj. R-squared:                  0.800\n",
       "Method:                 Least Squares   F-statistic:                     75.50\n",
       "Date:                Thu, 30 Apr 2015   Prob (F-statistic):           2.49e-30\n",
       "Time:                        18:09:42   Log-Likelihood:                -315.88\n",
       "No. Observations:                  94   AIC:                             643.8\n",
       "Df Residuals:                      88   BIC:                             659.0\n",
       "Df Model:                           5                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "const        -42.1069      8.990     -4.684      0.000       -59.972   -24.242\n",
       "x2             3.4714      0.467      7.428      0.000         2.543     4.400\n",
       "x3             0.2391      0.076      3.163      0.002         0.089     0.389\n",
       "x5             0.2787      0.078      3.582      0.001         0.124     0.433\n",
       "x4             0.6770      0.195      3.475      0.001         0.290     1.064\n",
       "x7            -2.5913      0.832     -3.115      0.002        -4.245    -0.938\n",
       "==============================================================================\n",
       "Omnibus:                        5.624   Durbin-Watson:                   1.718\n",
       "Prob(Omnibus):                  0.060   Jarque-Bera (JB):                3.905\n",
       "Skew:                           0.351   Prob(JB):                        0.142\n",
       "Kurtosis:                       2.290   Cond. No.                         620.\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ncaa.data2.txt',delim_whitespace=True)\n",
    "x = df.ix[:,0:19]\n",
    "y = df.ix[:,19]\n",
    "fsr_fast_vectorized(x,y)[0].summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
